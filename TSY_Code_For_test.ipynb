{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908453df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mgpu22              \u001b[m  Tue Jun  4 21:42:31 2024  \u001b[1m\u001b[30m550.54.15\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 83¬∞C\u001b[m, \u001b[1m\u001b[32m 34 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7418\u001b[m / \u001b[33m49140\u001b[m MB | \u001b[1m\u001b[30mwangyi\u001b[m(\u001b[33m7410M\u001b[m)\r\n",
      "\u001b[m ‚îî‚îÄ 1084644 \u001b[m(\u001b[32m  90%\u001b[m, \u001b[33m5222MB\u001b[m)\u001b[m: \u001b[324m\u001b[324m\u001b[36mpython\u001b[324m /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/main.py --beam_size 4 --exp_name VGAMTtestDE --dump_path /raid_elmo/home/lr/wangyi/temp --features_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/mdetr_features,/home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/clip_features --features_type mdetr+clip --data_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kNofilter --data_mix_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kNofilter --features_mix_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/mdetr_features,/home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/clip_features --src_lang en --tgt_lang de --dropout 0.4 --batch_size 32 --optimizer adam_inverse_sqrt,lr=0.00001,warmup_updates=2000 --epoch_size 116000 --eval_bleu true --max_epoch 5000 --max_len 80 --num_workers 12 --stopping_criterion valid_en-de_mt_bleu,10 --validation_metrics valid_en-de_mt_bleu --iter_seed 1357911 --other_seed 1357911 --smoothing 0.1 --save_periodic 40 --cache_dir /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/huggingface/ --amp -1 --fp16 False --accumulate_gradients 4 --reload_model /raid_elmo/home/lr/wangyi/MMT/VGAMT/VGAMTFINETUNE/best-valid_en-de_mt_bleu.pth --multimodal_model --adapters --encoder_attn_mask_text_only --freeze_text_parameters --start_new_xp_from_ckpt --mix_xp 0.5 --prob_mask_text 0.25 --guided_self_attention --min_epoch 80\u001b[m\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 82¬∞C\u001b[m, \u001b[1m\u001b[32m 94 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7220\u001b[m / \u001b[33m49140\u001b[m MB | \u001b[1m\u001b[30mwangyi\u001b[m(\u001b[33m7214M\u001b[m)\r\n",
      "\u001b[m ‚îî‚îÄ 1086288 \u001b[m(\u001b[32m 389%\u001b[m, \u001b[33m5855MB\u001b[m)\u001b[m: \u001b[324m\u001b[324m\u001b[36mpython\u001b[324m /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/main.py --beam_size 4 --exp_name VGAMTtestFR --dump_path /raid_elmo/home/lr/wangyi/temp --features_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/mdetr_features,/home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/clip_features --features_type mdetr+clip --data_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kNofilter --data_mix_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kNofilter --features_mix_path /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/mdetr_features,/home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/data/multi30kAttribute/features/clip_features --src_lang en --tgt_lang fr --dropout 0.4 --batch_size 32 --optimizer adam_inverse_sqrt,lr=0.00001,warmup_updates=2000 --epoch_size 116000 --eval_bleu true --max_epoch 5000 --max_len 80 --num_workers 12 --stopping_criterion valid_en-fr_mt_bleu,10 --validation_metrics valid_en-fr_mt_bleu --iter_seed 182364 --other_seed 182364 --smoothing 0.1 --save_periodic 40 --cache_dir /home/lr/wangyi/EMNLP2024/MMT/baseline/VGAMT/huggingface/ --amp -1 --fp16 False --accumulate_gradients 4 --reload_model /raid_elmo/home/lr/wangyi/MMT/VGAMT/VGAMTFINETUNE/best-valid_en-fr_mt_bleu.pth --multimodal_model --adapters --encoder_attn_mask_text_only --freeze_text_parameters --start_new_xp_from_ckpt --mix_xp 0.5 --prob_mask_text 0.25 --guided_self_attention --min_epoch 80\u001b[m\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 88¬∞C\u001b[m, \u001b[1m\u001b[32m 79 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m32238\u001b[m / \u001b[33m49140\u001b[m MB | \u001b[1m\u001b[30mwatanabe\u001b[m(\u001b[33m32232M\u001b[m)\r\n",
      "\u001b[m ‚îî‚îÄ 3250228 \u001b[m(\u001b[32m 100%\u001b[m, \u001b[33m3039MB\u001b[m)\u001b[m: \u001b[324m\u001b[324m/home/lr/watanabe/.pyenv/versions/automatically-evaluate-pds/bin/\u001b[36mpython\u001b[324m /home/lr/watanabe/projects/AutomaticallyEvaluatePDS/src/evaluator/evaluate.py --target_method CEPDS --evaluate_method Proposed --summary_dir /home/lr/watanabe/projects/AutomaticallyEvaluatePDS/experiment_data/generated_summary/ef8d9c66aba5c574597833b398deb19373818f46 --model_name_or_path microsoft/Phi-3-medium-4k-instruct --prompt_cls ChatStylePrompt --loadable_doc_json_path_list_for_cepds /home/lr/watanabe/projects/AutomaticallyEvaluatePDS/data/grouplens/doc_bbc_news.json /home/lr/watanabe/projects/AutomaticallyEvaluatePDS/data/grouplens/doc_bbc_cnn.json /home/lr/watanabe/projects/AutomaticallyEvaluatePDS/data/grouplens/guardian.json /home/lr/watanabe/projects/AutomaticallyEvaluatePDS/data/grouplens/washington_post.json --target_domains_for_cepds bbc bbc_news cnn guardian washington_post --completion_max_new_tokens 1024 --no_split --including_tags --exclude_titles --data_loader_do_truncate --data_loader_max_document_length 1024 --data_loader_min_bookmarked_user_num 5 --data_loader_min_user_bookmark_num 5\u001b[m\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA RTX A6000\u001b[m |\u001b[1m\u001b[31m 67¬∞C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    0\u001b[m / \u001b[33m49140\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd367955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf03177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lr/tanshiyin/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_4013361/3356187376.py:11: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "/home/lr/tanshiyin/anaconda3/envs/mds/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LEDForConditionalGeneration,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216abb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂèëÁé∞ 1 ‰∏™ÂèØÁî®ÁöÑ GPU:\n",
      "GPU 0: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"ÂèëÁé∞ {device_count} ‰∏™ÂèØÁî®ÁöÑ GPU:\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"Êú™ÂèëÁé∞ÂèØÁî®ÁöÑ GPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4321f788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lr/tanshiyin/anaconda3/envs/mds/lib/python3.9/site-packages/datasets/load.py:1429: FutureWarning: The repository for multi_news contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/multi_news\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset=load_dataset('multi_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91d9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7d214588",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 10\n",
    "item_embeddings = torch.rand(num_items, 4)\n",
    "item_embeddings_static = item_embeddings.clone()\n",
    "predicted_item_embedding = torch.rand(2,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35501287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "7 7\n",
      "8 8\n",
      "9 9\n"
     ]
    }
   ],
   "source": [
    "for item_id_idx, item_id in enumerate(range(num_items)):\n",
    "    print(item_id, item_id_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21486e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data_idx = random.choices(range(len(dataset['test'])),k=10)\n",
    "# data_idx = range(len(dataset['test']))\n",
    "dataset_small = dataset['test'].select(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ba87e9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â≠óÁ¨¶‰∏≤‰∏≠ÂåÖÂê´ Associated Press\n",
      "Â≠óÁ¨¶‰∏≤‰∏≠ÂåÖÂê´ FILE -\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of predictions (36) and references (1698)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 19\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mÂ≠óÁ¨¶‰∏≤‰∏≠ÂåÖÂê´ FILE -\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         else:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#             print(\"Â≠óÁ¨¶‰∏≤‰∏≠‰∏çÂåÖÂê´ Associated Press\")\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#     find(\"(Associated Press)\")\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     score\u001b[38;5;241m=\u001b[39m\u001b[43mrouge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_docs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_stemmer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mds/lib/python3.9/site-packages/datasets/metric.py:443\u001b[0m, in \u001b[0;36mMetric.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures}\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mds/lib/python3.9/site-packages/datasets/metric.py:520\u001b[0m, in \u001b[0;36mMetric.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    515\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m     )\n\u001b[0;32m--> 520\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of predictions (36) and references (1698)"
     ]
    }
   ],
   "source": [
    "for doc_target in dataset_small:\n",
    "    docs = doc_target['document']\n",
    "    target = doc_target['summary']\n",
    "    \n",
    "    all_docs = docs.split(\"|||||\")\n",
    "    for i, doc in enumerate(all_docs):\n",
    "#         doc = doc.replace(\"\\n\", \" \")\n",
    "#         doc = \" \".join(doc.split())\n",
    "        all_docs[i] = doc\n",
    "        if doc.find(\"Associated Press\") != -1:\n",
    "            print(\"Â≠óÁ¨¶‰∏≤‰∏≠ÂåÖÂê´ Associated Press\")\n",
    "        if doc.find(\"FILE -\") != -1:\n",
    "            print(\"Â≠óÁ¨¶‰∏≤‰∏≠ÂåÖÂê´ FILE -\")\n",
    "#         else:\n",
    "#             print(\"Â≠óÁ¨¶‰∏≤‰∏≠‰∏çÂåÖÂê´ Associated Press\")\n",
    "    \n",
    "#     find(\"(Associated Press)\")\n",
    "    \n",
    "    score=rouge.compute(predictions=all_docs[0], references=target, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f0baa",
   "metadata": {},
   "source": [
    "### DocumentÂíåtargetÁöÑÁõ∏‰ººÂ∫¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e1538f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 0\n",
    "\n",
    "docs = dataset['test'][doc_idx]['document']\n",
    "target = dataset['test'][doc_idx]['summary']\n",
    "\n",
    "all_docs = docs.split(\"|||||\")\n",
    "for i, doc in enumerate(all_docs):\n",
    "    doc = doc.replace(\"\\n\", \" \")\n",
    "    doc = \" \".join(doc.split())\n",
    "    all_docs[i] = doc\n",
    "\n",
    "num_docs = len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ba65b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e933d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 0.16864608076009502\n",
      "Jaccard Similarity: 0.17391304347826086\n"
     ]
    }
   ],
   "source": [
    "# Jaccard Áõ∏‰ººÂ∫¶ËÆ°ÁÆótargetÂíådocs‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "# Â∞ÜÊñáÊú¨ËΩ¨Êç¢‰∏∫n-gramÈõÜÂêà\n",
    "vectorizer = CountVectorizer().fit_transform(all_docs+[target])\n",
    "vectors = vectorizer.toarray()\n",
    "\n",
    "for i in range(num_docs):\n",
    "    vectors[i][vectors[i]>0] = 1\n",
    "vectors[-1][vectors[-1]>0] = 1\n",
    "\n",
    "# ËÆ°ÁÆóJaccardÁõ∏‰ººÂ∫¶\n",
    "for i in range(num_docs):\n",
    "    jaccard_sim = jaccard_score(vectors[i], vectors[-1], average='binary')\n",
    "    print(\"Jaccard Similarity:\", jaccard_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d1d174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.6288669160994215\n",
      "Cosine Similarity: 0.6330491122822253\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Â∞ÜÊñáÊú¨ÂêëÈáèÂåñ\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(all_docs+[target])\n",
    "\n",
    "# ËÆ°ÁÆó‰ΩôÂº¶Áõ∏‰ººÂ∫¶\n",
    "for i in range(num_docs):\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[-1])\n",
    "    print(\"Cosine Similarity:\", cosine_sim[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6a1dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance: 4395\n",
      "Levenshtein Distance: 4356\n"
     ]
    }
   ],
   "source": [
    "# ÁºñËæëË∑ùÁ¶ªÔºàLevenshteinË∑ùÁ¶ªÔºâ\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "# ËÆ°ÁÆóLevenshteinË∑ùÁ¶ª\n",
    "for i in range(num_docs):\n",
    "    levenshtein_dist = Levenshtein.distance(all_docs[i], target)\n",
    "    print(\"Levenshtein Distance:\", levenshtein_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1800d9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0022353499841904303\n",
      "BLEU Score: 0.0023272010157348983\n"
     ]
    }
   ],
   "source": [
    "# BLEUÂàÜÊï∞\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "# Á§∫‰æãÊñáÊú¨\n",
    "reference = \"The cat sat on the mat\".split()\n",
    "candidate = \"The cat is on the mat\".split()\n",
    "\n",
    "# ‰ΩøÁî®Âπ≥ÊªëÂáΩÊï∞ÔºåÂπ≥ÊªëÂáΩÊï∞ÂèØ‰ª•Â§ÑÁêÜÊ≤°ÊúâÈáçÂè†ÁöÑÊÉÖÂÜµÔºåÈÄöËøáÂØπ0ËÆ°Êï∞ÁöÑn-gramÊ∑ªÂä†‰∏Ä‰∏™Â∞èÁöÑÂπ≥ÊªëÂÄºÊù•ÈÅøÂÖçÂàÜÊØç‰∏∫0„ÄÇ\n",
    "smooth = SmoothingFunction()\n",
    "\n",
    "for i in range(num_docs):\n",
    "    reference = all_docs[i].split()\n",
    "    candidate = target.split()\n",
    "    # ËÆ°ÁÆóBLEUÂàÜÊï∞\n",
    "    bleu_score = sentence_bleu([reference], candidate, smoothing_function=smooth.method1)\n",
    "    print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e422bf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding Cosine Similarity: 0.8549716472625732\n",
      "Sentence Embedding Cosine Similarity: 0.8432109355926514\n"
     ]
    }
   ],
   "source": [
    "# Âè•ÂêëÈáèÁõ∏‰ººÂ∫¶ sentence transformer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑÂè•ÂêëÈáèÊ®°Âûã\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "# ËÆ°ÁÆóÊñáÊú¨ÁöÑÂè•ÂêëÈáè\n",
    "for i in range(num_docs):\n",
    "    embedding1 = model.encode(all_docs[i], convert_to_tensor=True)\n",
    "    embedding2 = model.encode(target, convert_to_tensor=True)\n",
    "\n",
    "    # ËÆ°ÁÆó‰ΩôÂº¶Áõ∏‰ººÂ∫¶\n",
    "    cosine_sim = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    print(\"Sentence Embedding Cosine Similarity:\", cosine_sim.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ece9e",
   "metadata": {},
   "source": [
    "### Document‰∏≠ÁöÑsentenceÂíåtargetÁöÑÁõ∏‰ººÂ∫¶\n",
    "\n",
    "Âè•Â≠êÂàÜÂâ≤ÊñπÂºèÔºö\n",
    "1. Áõ¥Êé•Áî®'.'ËøõË°åÂàÜÂâ≤Ôºå‰ºöÊääÊüê‰∫õÂåÖÂê´'.'ÁöÑÂçïËØçÂàÜÂâ≤Ôºå‰∏çÂáÜÁ°Æ\n",
    "2. Áî®nltk.sent_tokenize(text)ÊñπÂºèÔºå‰∏ç‰ºöÊääÂçïËØçÂàÜÂâ≤Ôºå‰ΩÜÊòØ‰ºöÊää'T.I.'ÂΩì‰ΩúÂÆåÊï¥ÁöÑÂè•Â≠êÔºåËæÉÂáÜÁ°Æ\n",
    "3. ‰ΩøÁî®spacyÁöÑÊ®°ÂûãÂàÜÂâ≤Ôºå‰∏ç‰ºöÊää'T.I.'ÂΩì‰ΩúÂÆåÊï¥Âè•Â≠êÔºåÁõÆÂâç‰ΩøÁî®, ‰ΩÜÊòØ‰ºöÊää'Washington:'ÂΩì‰ΩúÂÆåÊï¥Âè•Â≠êÔºõÂ∞ùËØï‰ΩøÁî®Êõ¥Â§ßÁöÑÊ®°ÂûãÔºöen_core_web_trf, en_core_web_lgÔºå‰ªéhttps://spacy.io/models/en#en_core_web_sm\n",
    "``` python\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.disable_pipe(\"parser\")\n",
    "nlp.enable_pipe(\"senter\") # ËøôÁßçËÆæÂÆöÊõ¥Âø´ÔºåËÄå‰∏îÊõ¥ÂáÜ\n",
    "# Â§ÑÁêÜÊñáÊú¨\n",
    "doc = nlp(text)\n",
    "# Âè•Â≠êÂàÜÂâ≤\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b36570e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Âä†ËΩΩËã±ÊñáÊ®°ÂûãÔºåÁî®‰∫éÂè•Â≠êÂàÜÂâ≤\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "nlp.disable_pipe(\"parser\")\n",
    "nlp.enable_pipe(\"senter\")\n",
    "\n",
    "# import en_core_web_trf\n",
    "# nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837e42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = 0\n",
    "\n",
    "docs = dataset['test'][doc_idx]['document']\n",
    "target = dataset['test'][doc_idx]['summary']\n",
    "\n",
    "all_docs = docs.split(\"|||||\")\n",
    "for i, doc in enumerate(all_docs):\n",
    "#     doc = doc.replace(\"\\n\", \" \")\n",
    "#     doc = \" \".join(doc.split())\n",
    "    all_docs[i] = doc\n",
    "\n",
    "num_docs = len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e813f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_senteces = []\n",
    "senteces_id_of_doc = []\n",
    "for i in range(num_docs):\n",
    "    doc = all_docs[i].replace(\"(Associated Press)\", \"\").replace(\"FILE -\", \"\")\n",
    "    # Â§ÑÁêÜÊñáÊú¨\n",
    "    doc = nlp(doc)\n",
    "    # Âè•Â≠êÂàÜÂâ≤\n",
    "    sentences = [\" \".join(sent.text.replace('\\n', ' ').split()) for sent in doc.sents]\n",
    "    all_senteces.extend(sentences)\n",
    "    if len(senteces_id_of_doc)==0:\n",
    "        senteces_id_of_doc.append(len(sentences))\n",
    "    else:\n",
    "        senteces_id_of_doc.append(len(sentences)+senteces_id_of_doc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6c0c913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 sent 0-Jaccard Similarity: 0.17054263565891473\n",
      "doc 0 sent 1-Jaccard Similarity: 0.09009009009009009\n",
      "doc 0 sent 2-Jaccard Similarity: 0.03418803418803419\n",
      "doc 0 sent 3-Jaccard Similarity: 0.1282051282051282\n",
      "doc 0 sent 4-Jaccard Similarity: 0.16521739130434782\n",
      "doc 0 sent 5-Jaccard Similarity: 0.06779661016949153\n",
      "doc 0 sent 6-Jaccard Similarity: 0.136\n",
      "doc 0 sent 7-Jaccard Similarity: 0.061946902654867256\n",
      "doc 0 sent 8-Jaccard Similarity: 0.04065040650406504\n",
      "doc 0 sent 9-Jaccard Similarity: 0.12396694214876033\n",
      "doc 0 sent 10-Jaccard Similarity: 0.049586776859504134\n",
      "doc 0 sent 11-Jaccard Similarity: 0.05128205128205128\n",
      "doc 0 sent 12-Jaccard Similarity: 0.06542056074766354\n",
      "doc 0 sent 13-Jaccard Similarity: 0.1103448275862069\n",
      "doc 0 sent 14-Jaccard Similarity: 0.09322033898305085\n",
      "doc 0 sent 15-Jaccard Similarity: 0.02608695652173913\n",
      "doc 0 sent 16-Jaccard Similarity: 0.04132231404958678\n",
      "doc 0 sent 17-Jaccard Similarity: 0.058823529411764705\n",
      "doc 0 sent 18-Jaccard Similarity: 0.04504504504504504\n",
      "doc 0 sent 19-Jaccard Similarity: 0.041666666666666664\n",
      "doc 0 sent 20-Jaccard Similarity: 0.04201680672268908\n",
      "doc 0 sent 21-Jaccard Similarity: 0.061946902654867256\n",
      "doc 0 sent 22-Jaccard Similarity: 0.08333333333333333\n",
      "doc 0 sent 23-Jaccard Similarity: 0.02702702702702703\n",
      "doc 0 sent 24-Jaccard Similarity: 0.034482758620689655\n",
      "doc 0 sent 25-Jaccard Similarity: 0.041666666666666664\n",
      "doc 0 sent 26-Jaccard Similarity: 0.009174311926605505\n",
      "doc 0 sent 27-Jaccard Similarity: 0.08928571428571429\n",
      "doc 0 sent 28-Jaccard Similarity: 0.009345794392523364\n",
      "doc 0 sent 29-Jaccard Similarity: 0.01818181818181818\n",
      "doc 0 sent 30-Jaccard Similarity: 0.06569343065693431\n",
      "doc 0 sent 31-Jaccard Similarity: 0.008695652173913044\n",
      "doc 0 sent 32-Jaccard Similarity: 0.02608695652173913\n",
      "doc 0 sent 33-Jaccard Similarity: 0.0423728813559322\n",
      "doc 1 sent 34-Jaccard Similarity: 0.1774193548387097\n",
      "doc 1 sent 35-Jaccard Similarity: 0.0990990990990991\n",
      "doc 1 sent 36-Jaccard Similarity: 0.03418803418803419\n",
      "doc 1 sent 37-Jaccard Similarity: 0.1282051282051282\n",
      "doc 1 sent 38-Jaccard Similarity: 0.16521739130434782\n",
      "doc 1 sent 39-Jaccard Similarity: 0.06779661016949153\n",
      "doc 1 sent 40-Jaccard Similarity: 0.136\n",
      "doc 1 sent 41-Jaccard Similarity: 0.061946902654867256\n",
      "doc 1 sent 42-Jaccard Similarity: 0.040983606557377046\n",
      "doc 1 sent 43-Jaccard Similarity: 0.12396694214876033\n",
      "doc 1 sent 44-Jaccard Similarity: 0.049586776859504134\n",
      "doc 1 sent 45-Jaccard Similarity: 0.05128205128205128\n",
      "doc 1 sent 46-Jaccard Similarity: 0.06542056074766354\n",
      "doc 1 sent 47-Jaccard Similarity: 0.10344827586206896\n",
      "doc 1 sent 48-Jaccard Similarity: 0.09322033898305085\n",
      "doc 1 sent 49-Jaccard Similarity: 0.02608695652173913\n",
      "doc 1 sent 50-Jaccard Similarity: 0.04132231404958678\n",
      "doc 1 sent 51-Jaccard Similarity: 0.058823529411764705\n",
      "doc 1 sent 52-Jaccard Similarity: 0.04504504504504504\n",
      "doc 1 sent 53-Jaccard Similarity: 0.041666666666666664\n",
      "doc 1 sent 54-Jaccard Similarity: 0.04201680672268908\n",
      "doc 1 sent 55-Jaccard Similarity: 0.061946902654867256\n",
      "doc 1 sent 56-Jaccard Similarity: 0.08333333333333333\n",
      "doc 1 sent 57-Jaccard Similarity: 0.02702702702702703\n",
      "doc 1 sent 58-Jaccard Similarity: 0.034482758620689655\n",
      "doc 1 sent 59-Jaccard Similarity: 0.04201680672268908\n",
      "doc 1 sent 60-Jaccard Similarity: 0.10344827586206896\n",
      "doc 1 sent 61-Jaccard Similarity: 0.03571428571428571\n",
      "doc 1 sent 62-Jaccard Similarity: 0.06569343065693431\n",
      "doc 1 sent 63-Jaccard Similarity: 0.008695652173913044\n",
      "doc 1 sent 64-Jaccard Similarity: 0.034782608695652174\n",
      "doc 1 sent 65-Jaccard Similarity: 0.0423728813559322\n"
     ]
    }
   ],
   "source": [
    "# Jaccard Áõ∏‰ººÂ∫¶ËÆ°ÁÆótargetÂíådocs‰πãÈó¥ÁöÑÁõ∏‰ººÂ∫¶\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "\n",
    "# all_senteces = []\n",
    "# for i in range(num_docs):\n",
    "#     sentences = nltk.sent_tokenize(all_docs[i])\n",
    "#     all_senteces.extend(sentences)\n",
    "\n",
    "\n",
    "    \n",
    "# Â∞ÜÊñáÊú¨ËΩ¨Êç¢‰∏∫n-gramÈõÜÂêà\n",
    "vectorizer = CountVectorizer().fit_transform(all_senteces+[target])\n",
    "vectors = vectorizer.toarray()\n",
    "\n",
    "for i in range(len(vectors)):\n",
    "    vectors[i][vectors[i]>0] = 1\n",
    "\n",
    "# ËÆ°ÁÆóJaccardÁõ∏‰ººÂ∫¶\n",
    "doc_id = 0\n",
    "for i in range(len(all_senteces)):\n",
    "    \n",
    "    if i == senteces_id_of_doc[doc_id]:\n",
    "        doc_id += 1\n",
    "    \n",
    "    jaccard_sim = jaccard_score(vectors[i], vectors[-1], average='binary')\n",
    "    print(f\"doc {doc_id} sent {i}-Jaccard Similarity:\", jaccard_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f7127a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 sent 0-Cosine Similarity: 0.24102418098008926\n",
      "doc 0 sent 1-Cosine Similarity: 0.23391266613631823\n",
      "doc 0 sent 2-Cosine Similarity: 0.07539999655241314\n",
      "doc 0 sent 3-Cosine Similarity: 0.24212850187007307\n",
      "doc 0 sent 4-Cosine Similarity: 0.31950391731366007\n",
      "doc 0 sent 5-Cosine Similarity: 0.1797630735667592\n",
      "doc 0 sent 6-Cosine Similarity: 0.25847237481195035\n",
      "doc 0 sent 7-Cosine Similarity: 0.16996419229512022\n",
      "doc 0 sent 8-Cosine Similarity: 0.15341846023710742\n",
      "doc 0 sent 9-Cosine Similarity: 0.3253832925427495\n",
      "doc 0 sent 10-Cosine Similarity: 0.12432858888706294\n",
      "doc 0 sent 11-Cosine Similarity: 0.16739494375604586\n",
      "doc 0 sent 12-Cosine Similarity: 0.15355150202098913\n",
      "doc 0 sent 13-Cosine Similarity: 0.19549630373574334\n",
      "doc 0 sent 14-Cosine Similarity: 0.21216530513647738\n",
      "doc 0 sent 15-Cosine Similarity: 0.11667598705079932\n",
      "doc 0 sent 16-Cosine Similarity: 0.08548231652024595\n",
      "doc 0 sent 17-Cosine Similarity: 0.13809644520701314\n",
      "doc 0 sent 18-Cosine Similarity: 0.0828451291993201\n",
      "doc 0 sent 19-Cosine Similarity: 0.11590856974304961\n",
      "doc 0 sent 20-Cosine Similarity: 0.12060134788433567\n",
      "doc 0 sent 21-Cosine Similarity: 0.15215422083280244\n",
      "doc 0 sent 22-Cosine Similarity: 0.20148170284723782\n",
      "doc 0 sent 23-Cosine Similarity: 0.10440020854618501\n",
      "doc 0 sent 24-Cosine Similarity: 0.06296630062136835\n",
      "doc 0 sent 25-Cosine Similarity: 0.06330689120322054\n",
      "doc 0 sent 26-Cosine Similarity: 0.024498992079324634\n",
      "doc 0 sent 27-Cosine Similarity: 0.19944980835363155\n",
      "doc 0 sent 28-Cosine Similarity: 0.02888559489445314\n",
      "doc 0 sent 29-Cosine Similarity: 0.055172391704326806\n",
      "doc 0 sent 30-Cosine Similarity: 0.1314522387333815\n",
      "doc 0 sent 31-Cosine Similarity: 0.02391626414265239\n",
      "doc 0 sent 32-Cosine Similarity: 0.04878564164092376\n",
      "doc 0 sent 33-Cosine Similarity: 0.06862337188407987\n",
      "doc 1 sent 34-Cosine Similarity: 0.2622394890883862\n",
      "doc 1 sent 35-Cosine Similarity: 0.24719936893558836\n",
      "doc 1 sent 36-Cosine Similarity: 0.07539999655241314\n",
      "doc 1 sent 37-Cosine Similarity: 0.24212850187007307\n",
      "doc 1 sent 38-Cosine Similarity: 0.31950391731366007\n",
      "doc 1 sent 39-Cosine Similarity: 0.1606856300094091\n",
      "doc 1 sent 40-Cosine Similarity: 0.25847237481195035\n",
      "doc 1 sent 41-Cosine Similarity: 0.16996419229512022\n",
      "doc 1 sent 42-Cosine Similarity: 0.15844229783269972\n",
      "doc 1 sent 43-Cosine Similarity: 0.3253832925427495\n",
      "doc 1 sent 44-Cosine Similarity: 0.12432858888706294\n",
      "doc 1 sent 45-Cosine Similarity: 0.16739494375604586\n",
      "doc 1 sent 46-Cosine Similarity: 0.15355150202098913\n",
      "doc 1 sent 47-Cosine Similarity: 0.18564324476721036\n",
      "doc 1 sent 48-Cosine Similarity: 0.21216530513647738\n",
      "doc 1 sent 49-Cosine Similarity: 0.11667598705079932\n",
      "doc 1 sent 50-Cosine Similarity: 0.08548231652024595\n",
      "doc 1 sent 51-Cosine Similarity: 0.13809644520701314\n",
      "doc 1 sent 52-Cosine Similarity: 0.0828451291993201\n",
      "doc 1 sent 53-Cosine Similarity: 0.11590856974304961\n",
      "doc 1 sent 54-Cosine Similarity: 0.12060134788433567\n",
      "doc 1 sent 55-Cosine Similarity: 0.15215422083280244\n",
      "doc 1 sent 56-Cosine Similarity: 0.20148170284723782\n",
      "doc 1 sent 57-Cosine Similarity: 0.10440020854618501\n",
      "doc 1 sent 58-Cosine Similarity: 0.06296630062136835\n",
      "doc 1 sent 59-Cosine Similarity: 0.06684931715321048\n",
      "doc 1 sent 60-Cosine Similarity: 0.19081674773653892\n",
      "doc 1 sent 61-Cosine Similarity: 0.07444832970264362\n",
      "doc 1 sent 62-Cosine Similarity: 0.1314522387333815\n",
      "doc 1 sent 63-Cosine Similarity: 0.02391626414265239\n",
      "doc 1 sent 64-Cosine Similarity: 0.06615924360743808\n",
      "doc 1 sent 65-Cosine Similarity: 0.06862337188407987\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# all_senteces = []\n",
    "# for i in range(num_docs):\n",
    "#     sentences = nltk.sent_tokenize(all_docs[i])\n",
    "#     all_senteces.extend(sentences)\n",
    "\n",
    "\n",
    "\n",
    "# Â∞ÜÊñáÊú¨ÂêëÈáèÂåñ\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(all_senteces+[target])\n",
    "\n",
    "# ËÆ°ÁÆó‰ΩôÂº¶Áõ∏‰ººÂ∫¶\n",
    "doc_id = 0\n",
    "for i in range(len(all_senteces)):\n",
    "    \n",
    "    if i == senteces_id_of_doc[doc_id]:\n",
    "        doc_id += 1\n",
    "    \n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[-1])\n",
    "    print(f\"doc {doc_id} sent {i}-Cosine Similarity:\", cosine_sim[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97aab802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 sent 0-Levenshtein Distance: 680\n",
      "doc 0 sent 1-Levenshtein Distance: 789\n",
      "doc 0 sent 2-Levenshtein Distance: 786\n",
      "doc 0 sent 3-Levenshtein Distance: 734\n",
      "doc 0 sent 4-Levenshtein Distance: 730\n",
      "doc 0 sent 5-Levenshtein Distance: 749\n",
      "doc 0 sent 6-Levenshtein Distance: 697\n",
      "doc 0 sent 7-Levenshtein Distance: 792\n",
      "doc 0 sent 8-Levenshtein Distance: 760\n",
      "doc 0 sent 9-Levenshtein Distance: 708\n",
      "doc 0 sent 10-Levenshtein Distance: 770\n",
      "doc 0 sent 11-Levenshtein Distance: 766\n",
      "doc 0 sent 12-Levenshtein Distance: 801\n",
      "doc 0 sent 13-Levenshtein Distance: 661\n",
      "doc 0 sent 14-Levenshtein Distance: 757\n",
      "doc 0 sent 15-Levenshtein Distance: 797\n",
      "doc 0 sent 16-Levenshtein Distance: 778\n",
      "doc 0 sent 17-Levenshtein Distance: 769\n",
      "doc 0 sent 18-Levenshtein Distance: 827\n",
      "doc 0 sent 19-Levenshtein Distance: 762\n",
      "doc 0 sent 20-Levenshtein Distance: 779\n",
      "doc 0 sent 21-Levenshtein Distance: 785\n",
      "doc 0 sent 22-Levenshtein Distance: 764\n",
      "doc 0 sent 23-Levenshtein Distance: 821\n",
      "doc 0 sent 24-Levenshtein Distance: 796\n",
      "doc 0 sent 25-Levenshtein Distance: 762\n",
      "doc 0 sent 26-Levenshtein Distance: 844\n",
      "doc 0 sent 27-Levenshtein Distance: 790\n",
      "doc 0 sent 28-Levenshtein Distance: 860\n",
      "doc 0 sent 29-Levenshtein Distance: 834\n",
      "doc 0 sent 30-Levenshtein Distance: 698\n",
      "doc 0 sent 31-Levenshtein Distance: 824\n",
      "doc 0 sent 32-Levenshtein Distance: 807\n",
      "doc 0 sent 33-Levenshtein Distance: 788\n",
      "doc 1 sent 34-Levenshtein Distance: 686\n",
      "doc 1 sent 35-Levenshtein Distance: 784\n",
      "doc 1 sent 36-Levenshtein Distance: 787\n",
      "doc 1 sent 37-Levenshtein Distance: 734\n",
      "doc 1 sent 38-Levenshtein Distance: 730\n",
      "doc 1 sent 39-Levenshtein Distance: 754\n",
      "doc 1 sent 40-Levenshtein Distance: 697\n",
      "doc 1 sent 41-Levenshtein Distance: 792\n",
      "doc 1 sent 42-Levenshtein Distance: 759\n",
      "doc 1 sent 43-Levenshtein Distance: 708\n",
      "doc 1 sent 44-Levenshtein Distance: 770\n",
      "doc 1 sent 45-Levenshtein Distance: 766\n",
      "doc 1 sent 46-Levenshtein Distance: 801\n",
      "doc 1 sent 47-Levenshtein Distance: 663\n",
      "doc 1 sent 48-Levenshtein Distance: 757\n",
      "doc 1 sent 49-Levenshtein Distance: 797\n",
      "doc 1 sent 50-Levenshtein Distance: 778\n",
      "doc 1 sent 51-Levenshtein Distance: 769\n",
      "doc 1 sent 52-Levenshtein Distance: 827\n",
      "doc 1 sent 53-Levenshtein Distance: 762\n",
      "doc 1 sent 54-Levenshtein Distance: 779\n",
      "doc 1 sent 55-Levenshtein Distance: 785\n",
      "doc 1 sent 56-Levenshtein Distance: 765\n",
      "doc 1 sent 57-Levenshtein Distance: 821\n",
      "doc 1 sent 58-Levenshtein Distance: 796\n",
      "doc 1 sent 59-Levenshtein Distance: 770\n",
      "doc 1 sent 60-Levenshtein Distance: 765\n",
      "doc 1 sent 61-Levenshtein Distance: 814\n",
      "doc 1 sent 62-Levenshtein Distance: 697\n",
      "doc 1 sent 63-Levenshtein Distance: 824\n",
      "doc 1 sent 64-Levenshtein Distance: 807\n",
      "doc 1 sent 65-Levenshtein Distance: 788\n"
     ]
    }
   ],
   "source": [
    "# ÁºñËæëË∑ùÁ¶ªÔºàLevenshteinË∑ùÁ¶ªÔºâ\n",
    "\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "# ËÆ°ÁÆóLevenshteinË∑ùÁ¶ª\n",
    "doc_id = 0\n",
    "for i in range(len(all_senteces)):\n",
    "    \n",
    "    if i == senteces_id_of_doc[doc_id]:\n",
    "        doc_id += 1\n",
    "    \n",
    "    levenshtein_dist = Levenshtein.distance(all_senteces[i], target)\n",
    "    print(f\"doc {doc_id} sent {i}-Levenshtein Distance:\", levenshtein_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269c53b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 sent 0-BLEU Score: 0.052516537134932935\n",
      "doc 0 sent 1-BLEU Score: 0.008922792017840799\n",
      "doc 0 sent 2-BLEU Score: 0.0017050434639569296\n",
      "doc 0 sent 3-BLEU Score: 0.004071048670237826\n",
      "doc 0 sent 4-BLEU Score: 0.024833537164633254\n",
      "doc 0 sent 5-BLEU Score: 0.004147175988212835\n",
      "doc 0 sent 6-BLEU Score: 0.006245303961616687\n",
      "doc 0 sent 7-BLEU Score: 0.003205995862201587\n",
      "doc 0 sent 8-BLEU Score: 0.0034873454219144554\n",
      "doc 0 sent 9-BLEU Score: 0.049667074329266515\n",
      "doc 0 sent 10-BLEU Score: 0.002027649818726467\n",
      "doc 0 sent 11-BLEU Score: 0.003605727923410432\n",
      "doc 0 sent 12-BLEU Score: 0.03773881547728682\n",
      "doc 0 sent 13-BLEU Score: 0.0042193278398767265\n",
      "doc 0 sent 14-BLEU Score: 0.004931851192351146\n",
      "doc 0 sent 15-BLEU Score: 0.003205995862201587\n",
      "doc 0 sent 16-BLEU Score: 0.0028216345864346703\n",
      "doc 0 sent 17-BLEU Score: 0.007096035310665079\n",
      "doc 0 sent 18-BLEU Score: 0.0018028639617052157\n",
      "doc 0 sent 19-BLEU Score: 0.003355507926125869\n",
      "doc 0 sent 20-BLEU Score: 0.0034873454219144554\n",
      "doc 0 sent 21-BLEU Score: 0.0019610784459558714\n",
      "doc 0 sent 22-BLEU Score: 0.003713479953254624\n",
      "doc 0 sent 23-BLEU Score: 0.0014337649366931587\n",
      "doc 0 sent 24-BLEU Score: 0.003205995862201587\n",
      "doc 0 sent 25-BLEU Score: 0.0015867217325385946\n",
      "doc 0 sent 26-BLEU Score: 0.0012056477955817465\n",
      "doc 0 sent 27-BLEU Score: 0.003812593089999409\n",
      "doc 0 sent 28-BLEU Score: 0.0012056477955817465\n",
      "doc 0 sent 29-BLEU Score: 0.0015867217325385946\n",
      "doc 0 sent 30-BLEU Score: 0.0042193278398767265\n",
      "doc 0 sent 31-BLEU Score: 0.0012056477955817465\n",
      "doc 0 sent 32-BLEU Score: 0.0014337649366931587\n",
      "doc 0 sent 33-BLEU Score: 0.0018028639617052157\n",
      "doc 1 sent 34-BLEU Score: 0.05316103239005418\n",
      "doc 1 sent 35-BLEU Score: 0.008922792017840799\n",
      "doc 1 sent 36-BLEU Score: 0.0017050434639569296\n",
      "doc 1 sent 37-BLEU Score: 0.004071048670237826\n",
      "doc 1 sent 38-BLEU Score: 0.024833537164633254\n",
      "doc 1 sent 39-BLEU Score: 0.00399039390019691\n",
      "doc 1 sent 40-BLEU Score: 0.006245303961616687\n",
      "doc 1 sent 41-BLEU Score: 0.003205995862201587\n",
      "doc 1 sent 42-BLEU Score: 0.0034873454219144554\n",
      "doc 1 sent 43-BLEU Score: 0.049667074329266515\n",
      "doc 1 sent 44-BLEU Score: 0.002027649818726467\n",
      "doc 1 sent 45-BLEU Score: 0.003605727923410432\n",
      "doc 1 sent 46-BLEU Score: 0.03773881547728682\n",
      "doc 1 sent 47-BLEU Score: 0.004147175988212835\n",
      "doc 1 sent 48-BLEU Score: 0.004931851192351146\n",
      "doc 1 sent 49-BLEU Score: 0.003205995862201587\n",
      "doc 1 sent 50-BLEU Score: 0.0028216345864346703\n",
      "doc 1 sent 51-BLEU Score: 0.007096035310665079\n",
      "doc 1 sent 52-BLEU Score: 0.0018028639617052157\n",
      "doc 1 sent 53-BLEU Score: 0.003355507926125869\n",
      "doc 1 sent 54-BLEU Score: 0.0034873454219144554\n",
      "doc 1 sent 55-BLEU Score: 0.0019610784459558714\n",
      "doc 1 sent 56-BLEU Score: 0.003713479953254624\n",
      "doc 1 sent 57-BLEU Score: 0.0014337649366931587\n",
      "doc 1 sent 58-BLEU Score: 0.003205995862201587\n",
      "doc 1 sent 59-BLEU Score: 0.0015867217325385946\n",
      "doc 1 sent 60-BLEU Score: 0.003904528774377159\n",
      "doc 1 sent 61-BLEU Score: 0.0017050434639569296\n",
      "doc 1 sent 62-BLEU Score: 0.0042193278398767265\n",
      "doc 1 sent 63-BLEU Score: 0.0012056477955817465\n",
      "doc 1 sent 64-BLEU Score: 0.0014337649366931587\n",
      "doc 1 sent 65-BLEU Score: 0.0018028639617052157\n"
     ]
    }
   ],
   "source": [
    "# BLEUÂàÜÊï∞\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "\n",
    "# ‰ΩøÁî®Âπ≥ÊªëÂáΩÊï∞ÔºåÂπ≥ÊªëÂáΩÊï∞ÂèØ‰ª•Â§ÑÁêÜÊ≤°ÊúâÈáçÂè†ÁöÑÊÉÖÂÜµÔºåÈÄöËøáÂØπ0ËÆ°Êï∞ÁöÑn-gramÊ∑ªÂä†‰∏Ä‰∏™Â∞èÁöÑÂπ≥ÊªëÂÄºÊù•ÈÅøÂÖçÂàÜÊØç‰∏∫0„ÄÇ\n",
    "smooth = SmoothingFunction()\n",
    "\n",
    "\n",
    "\n",
    "doc_id = 0\n",
    "for i in range(len(all_senteces)):\n",
    "    reference = all_senteces[i].split()\n",
    "    candidate = target.split()\n",
    "    \n",
    "    if i == senteces_id_of_doc[doc_id]:\n",
    "        doc_id += 1\n",
    "    \n",
    "    # ËÆ°ÁÆóBLEUÂàÜÊï∞\n",
    "    bleu_score = sentence_bleu([reference], candidate, smoothing_function=smooth.method1)\n",
    "    print(f\"doc {doc_id} sent {i}-BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f7ea806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 sent 0-Sentence Embedding Cosine Similarity: 0.7516170144081116\n",
      "doc 0 sent 1-Sentence Embedding Cosine Similarity: 0.6076449751853943\n",
      "doc 0 sent 2-Sentence Embedding Cosine Similarity: 0.5492553114891052\n",
      "doc 0 sent 3-Sentence Embedding Cosine Similarity: 0.5720685720443726\n",
      "doc 0 sent 4-Sentence Embedding Cosine Similarity: 0.718743085861206\n",
      "doc 0 sent 5-Sentence Embedding Cosine Similarity: 0.5036206841468811\n",
      "doc 0 sent 6-Sentence Embedding Cosine Similarity: 0.6540898084640503\n",
      "doc 0 sent 7-Sentence Embedding Cosine Similarity: 0.3216194808483124\n",
      "doc 0 sent 8-Sentence Embedding Cosine Similarity: 0.4208720922470093\n",
      "doc 0 sent 9-Sentence Embedding Cosine Similarity: 0.5073840022087097\n",
      "doc 0 sent 10-Sentence Embedding Cosine Similarity: 0.2851841151714325\n",
      "doc 0 sent 11-Sentence Embedding Cosine Similarity: 0.40695667266845703\n",
      "doc 0 sent 12-Sentence Embedding Cosine Similarity: 0.6262562274932861\n",
      "doc 0 sent 13-Sentence Embedding Cosine Similarity: 0.3923552930355072\n",
      "doc 0 sent 14-Sentence Embedding Cosine Similarity: 0.33049869537353516\n",
      "doc 0 sent 15-Sentence Embedding Cosine Similarity: 0.38296639919281006\n",
      "doc 0 sent 16-Sentence Embedding Cosine Similarity: 0.36300766468048096\n",
      "doc 0 sent 17-Sentence Embedding Cosine Similarity: 0.23144996166229248\n",
      "doc 0 sent 18-Sentence Embedding Cosine Similarity: 0.37580442428588867\n",
      "doc 0 sent 19-Sentence Embedding Cosine Similarity: 0.2912493348121643\n",
      "doc 0 sent 20-Sentence Embedding Cosine Similarity: 0.2697591185569763\n",
      "doc 0 sent 21-Sentence Embedding Cosine Similarity: 0.3447844386100769\n",
      "doc 0 sent 22-Sentence Embedding Cosine Similarity: 0.4850161671638489\n",
      "doc 0 sent 23-Sentence Embedding Cosine Similarity: 0.3884527385234833\n",
      "doc 0 sent 24-Sentence Embedding Cosine Similarity: 0.4507734179496765\n",
      "doc 0 sent 25-Sentence Embedding Cosine Similarity: 0.4609387516975403\n",
      "doc 0 sent 26-Sentence Embedding Cosine Similarity: 0.49279892444610596\n",
      "doc 0 sent 27-Sentence Embedding Cosine Similarity: 0.4992506206035614\n",
      "doc 0 sent 28-Sentence Embedding Cosine Similarity: 0.286185622215271\n",
      "doc 0 sent 29-Sentence Embedding Cosine Similarity: 0.322212815284729\n",
      "doc 0 sent 30-Sentence Embedding Cosine Similarity: 0.47985485196113586\n",
      "doc 0 sent 31-Sentence Embedding Cosine Similarity: 0.30548614263534546\n",
      "doc 0 sent 32-Sentence Embedding Cosine Similarity: 0.323362797498703\n",
      "doc 0 sent 33-Sentence Embedding Cosine Similarity: 0.4692564606666565\n",
      "doc 1 sent 34-Sentence Embedding Cosine Similarity: 0.7424992322921753\n",
      "doc 1 sent 35-Sentence Embedding Cosine Similarity: 0.6164596080780029\n",
      "doc 1 sent 36-Sentence Embedding Cosine Similarity: 0.5469348430633545\n",
      "doc 1 sent 37-Sentence Embedding Cosine Similarity: 0.5720685720443726\n",
      "doc 1 sent 38-Sentence Embedding Cosine Similarity: 0.718743085861206\n",
      "doc 1 sent 39-Sentence Embedding Cosine Similarity: 0.5013025999069214\n",
      "doc 1 sent 40-Sentence Embedding Cosine Similarity: 0.6540898084640503\n",
      "doc 1 sent 41-Sentence Embedding Cosine Similarity: 0.3216194808483124\n",
      "doc 1 sent 42-Sentence Embedding Cosine Similarity: 0.4210902452468872\n",
      "doc 1 sent 43-Sentence Embedding Cosine Similarity: 0.5073840022087097\n",
      "doc 1 sent 44-Sentence Embedding Cosine Similarity: 0.2851841151714325\n",
      "doc 1 sent 45-Sentence Embedding Cosine Similarity: 0.40695667266845703\n",
      "doc 1 sent 46-Sentence Embedding Cosine Similarity: 0.6262562274932861\n",
      "doc 1 sent 47-Sentence Embedding Cosine Similarity: 0.39086341857910156\n",
      "doc 1 sent 48-Sentence Embedding Cosine Similarity: 0.33049869537353516\n",
      "doc 1 sent 49-Sentence Embedding Cosine Similarity: 0.38296639919281006\n",
      "doc 1 sent 50-Sentence Embedding Cosine Similarity: 0.36300766468048096\n",
      "doc 1 sent 51-Sentence Embedding Cosine Similarity: 0.23144996166229248\n",
      "doc 1 sent 52-Sentence Embedding Cosine Similarity: 0.37580442428588867\n",
      "doc 1 sent 53-Sentence Embedding Cosine Similarity: 0.2912493348121643\n",
      "doc 1 sent 54-Sentence Embedding Cosine Similarity: 0.2697591185569763\n",
      "doc 1 sent 55-Sentence Embedding Cosine Similarity: 0.3447844386100769\n",
      "doc 1 sent 56-Sentence Embedding Cosine Similarity: 0.49862509965896606\n",
      "doc 1 sent 57-Sentence Embedding Cosine Similarity: 0.3884527385234833\n",
      "doc 1 sent 58-Sentence Embedding Cosine Similarity: 0.4507734179496765\n",
      "doc 1 sent 59-Sentence Embedding Cosine Similarity: 0.4468018710613251\n",
      "doc 1 sent 60-Sentence Embedding Cosine Similarity: 0.6283113360404968\n",
      "doc 1 sent 61-Sentence Embedding Cosine Similarity: 0.40787047147750854\n",
      "doc 1 sent 62-Sentence Embedding Cosine Similarity: 0.47340232133865356\n",
      "doc 1 sent 63-Sentence Embedding Cosine Similarity: 0.30548614263534546\n",
      "doc 1 sent 64-Sentence Embedding Cosine Similarity: 0.31184130907058716\n",
      "doc 1 sent 65-Sentence Embedding Cosine Similarity: 0.4692564606666565\n"
     ]
    }
   ],
   "source": [
    "# Âè•ÂêëÈáèÁõ∏‰ººÂ∫¶ sentence transformer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÁöÑÂè•ÂêëÈáèÊ®°Âûã\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "doc_id = 0\n",
    "# ËÆ°ÁÆóÊñáÊú¨ÁöÑÂè•ÂêëÈáè\n",
    "for i in range(len(all_senteces)):\n",
    "    embedding1 = model.encode(all_senteces[i], convert_to_tensor=True)\n",
    "    embedding2 = model.encode(target, convert_to_tensor=True)\n",
    "    \n",
    "    if i == senteces_id_of_doc[doc_id]:\n",
    "        doc_id += 1\n",
    "    # ËÆ°ÁÆó‰ΩôÂº¶Áõ∏‰ººÂ∫¶\n",
    "    cosine_sim = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    print(f\"doc {doc_id} sent {i}-Sentence Embedding Cosine Similarity:\", cosine_sim.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mds]",
   "language": "python",
   "name": "conda-env-mds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
