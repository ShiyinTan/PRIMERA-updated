{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d6cf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mgpu20                  \u001b[m  Sun Apr 21 18:21:31 2024  \u001b[1m\u001b[30m550.54.15\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mTesla V100-DGXS-32GB\u001b[m |\u001b[1m\u001b[31m 50°C\u001b[m, \u001b[32m  6 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 7081\u001b[m / \u001b[33m32768\u001b[m MB | \u001b[1m\u001b[30mtanshiyin\u001b[m(\u001b[33m4614M\u001b[m) \u001b[1m\u001b[30mkwonjingun\u001b[m(\u001b[33m2464M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mTesla V100-DGXS-32GB\u001b[m |\u001b[1m\u001b[31m 61°C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m10883\u001b[m / \u001b[33m32768\u001b[m MB | \u001b[1m\u001b[30mzym\u001b[m(\u001b[33m10880M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mTesla V100-DGXS-32GB\u001b[m |\u001b[31m 48°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m 1045\u001b[m / \u001b[33m32768\u001b[m MB | \u001b[1m\u001b[30mfaza.thirafi\u001b[m(\u001b[33m1042M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mTesla V100-DGXS-32GB\u001b[m |\u001b[31m 45°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    3\u001b[m / \u001b[33m32768\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "! gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f2f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 1 个可用的 GPU:\n",
      "GPU 0: Tesla V100-DGXS-32GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "cores = os.cpu_count()\n",
    "cores\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"发现 {device_count} 个可用的 GPU:\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"未发现可用的 GPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6608f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from primer_hf_main_modify import PRIMERSummarizer\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LEDForConditionalGeneration,\n",
    ")\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dfdd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self.__dict__[key] = value\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return f\"Attribute '{item}' not found.\"\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        try:\n",
    "            return self.__dict__[key]\n",
    "        except KeyError:\n",
    "            return f\"Attribute '{key}' not found.\"\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        self.__dict__[key] = value\n",
    "    \n",
    "    def __str__(self): # 作用于print输出\n",
    "        attrs = [f\"{key}={value}\" for key, value in self.__dict__.items()]\n",
    "        return f\"Namespace({', '.join(attrs)})\"\n",
    "    \n",
    "    def __repr__(self): # 作用于jupyter notebook表示\n",
    "        attrs = [f\"{key}={value}\" for key, value in self.__dict__.items()]\n",
    "        return f\"Namespace({', '.join(attrs)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd75d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(value):\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    if value.lower() in {'false', 'f', '0', 'no', 'n'}:\n",
    "        return False\n",
    "    elif value.lower() in {'true', 't', '1', 'yes', 'y'}:\n",
    "        return True\n",
    "    raise ValueError(f'{value} is not a valid boolean value')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "########################\n",
    "# Gneral\n",
    "parser.add_argument(\"--devices\", default=0, type=int, help=\"number of gpus to use\")\n",
    "parser.add_argument(\n",
    "    \"--accelerator\", default='gpu', type=str, help=\"Type of accelerator\"\n",
    ") # gpu\n",
    "parser.add_argument(\"--mode\", default=\"train\", choices=[\"train\", \"test\"])\n",
    "parser.add_argument(\n",
    "    \"--model_name\", default=\"primer\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--primer_path\", type=str, default=\"allenai/PRIMERA-multinews\", # ../PRIMERA/ # allenai/PRIMERA\n",
    ")\n",
    "parser.add_argument(\"--join_method\", type=str, default=\"tsy_design\") # concat_start_wdoc_global\n",
    "parser.add_argument(\n",
    "    \"--debug_mode\", action=\"store_true\", help=\"set true if to debug\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--compute_rouge\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether to compute rouge in validation steps\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--saveRouge\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether to compute rouge in validation steps\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--progress_bar_refresh_rate\", default=1, type=int)\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"./run_saves/tsy_join_method/\") # \"./pegasus/\"\n",
    "parser.add_argument(\"--ckpt_path\", type=str, default=None)\n",
    "parser.add_argument(\"--saveTopK\", default=3, type=int)\n",
    "parser.add_argument(\n",
    "    \"--resume_ckpt\",\n",
    "    type=str,\n",
    "    help=\"Path of a checkpoint to resume from\",\n",
    "    default=None,\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"../dataset/\")\n",
    "parser.add_argument(\"--dataset_name\", type=str, default=\"multi_news\") # arxiv\n",
    "parser.add_argument(\n",
    "    \"--num_workers\",\n",
    "    type=int,\n",
    "    default=4,\n",
    "    help=\"Number of workers to use for dataloader\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--batch_size\", default=4, type=int)\n",
    "parser.add_argument(\"--max_length_input\", default=4096, type=int)\n",
    "parser.add_argument(\"--max_length_tgt\", default=1024, type=int)\n",
    "parser.add_argument(\"--min_length_tgt\", default=0, type=int)\n",
    "parser.add_argument(\"--label_smoothing\", type=float, default=0.0, required=False)\n",
    "parser.add_argument(\n",
    "    \"--adafactor\", action=\"store_true\", help=\"Use adafactor optimizer\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--grad_ckpt\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Enable gradient checkpointing to save memory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--rand_seed\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"seed for random sampling, useful for few shot learning\",\n",
    ")\n",
    "\n",
    "########################\n",
    "# TSY added\n",
    "parser.add_argument(\"--permute_docs\", type=str_to_bool, default=False)\n",
    "\n",
    "########################\n",
    "# For training\n",
    "parser.add_argument(\n",
    "    \"--pretrained_model_path\", type=str, default=\"./pretrained_models/\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--limit_valid_batches\", type=int, default=None,\n",
    ")\n",
    "parser.add_argument(\"--lr\", type=float, default=3e-5, help=\"Maximum learning rate\")\n",
    "parser.add_argument(\n",
    "    \"--warmup_steps\", type=int, default=1000, help=\"Number of warmup steps\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--accum_data_per_step\", type=int, default=16, help=\"Number of data per step\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--total_steps\", type=int, default=-1, help=\"Number of steps to train\" # 50000\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_train_data\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"Number of training data, -1 for full dataset and any positive number indicates how many data to use\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--fix_lr\", action=\"store_true\", help=\"use fix learning rate\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--test_imediate\", action=\"store_true\", help=\"test on the best checkpoint\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fewshot\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether this is a run for few shot learning\",\n",
    ")\n",
    "########################\n",
    "# For testing\n",
    "parser.add_argument(\n",
    "    \"--limit_test_batches\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"Number of batches to test in the test mode.\",\n",
    ")\n",
    "parser.add_argument(\"--beam_size\", type=int, default=1, help=\"size of beam search\")\n",
    "parser.add_argument(\n",
    "    \"--length_penalty\",\n",
    "    type=float,\n",
    "    default=1,\n",
    "    help=\"length penalty of generated text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mask_num\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"Number of masks in the input of summarization data\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--test_batch_size\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"batch size for test, used in few shot evaluation.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--applyTriblck\",\n",
    "    action=\"store_true\",\n",
    "    help=\"whether apply trigram block in the evaluation phase\",\n",
    ")\n",
    "\n",
    "# args = parser.parse_args()  # Get pad token id\n",
    "args, unknown_args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373be33",
   "metadata": {},
   "source": [
    "## Convert to HF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7743ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_ckpt = \"run_saves/tsy_join_method_train_5/summ_checkpoints/step=33732-vloss=2.03-avgr=0.3146-v1.ckpt\"\n",
    "output_dir = \"./run_saves/hf_form_for_evaluate/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5d145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lr/tanshiyin/anaconda3/envs/mds/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = PRIMERSummarizer.load_from_checkpoint(resume_ckpt, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72b6a2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEDTokenizerFast(name_or_path='allenai/PRIMERA-multinews', vocab_size=50265, model_max_length=4096, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<doc-sep>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "\t50265: AddedToken(\"<doc-sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model\n",
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bcc1bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'num_beams': 5, 'no_repeat_ngram_size': 3}\n"
     ]
    }
   ],
   "source": [
    "model.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "061dda85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./run_saves/hf_form_for_evaluate/tokenizer_config.json',\n",
       " './run_saves/hf_form_for_evaluate/special_tokens_map.json',\n",
       " './run_saves/hf_form_for_evaluate/vocab.json',\n",
       " './run_saves/hf_form_for_evaluate/merges.txt',\n",
       " './run_saves/hf_form_for_evaluate/added_tokens.json',\n",
       " './run_saves/hf_form_for_evaluate/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed6f33c",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44d0d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lr/tanshiyin/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1909548/2874987259.py:9: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n",
      "/home/lr/tanshiyin/anaconda3/envs/mds/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LEDForConditionalGeneration,\n",
    "    AutoModelForSeq2SeqLM,\n",
    ")\n",
    "from datasets import load_dataset, load_metric\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "451499bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 50265\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PRIMER_path = \"./run_saves/hf_form_for_evaluate/\"\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(PRIMER_path)\n",
    "# MODEL = LEDForConditionalGeneration.from_pretrained(PRIMER_path)\n",
    "MODEL = LEDForConditionalGeneration.from_pretrained(PRIMER_path).to(device)\n",
    "\n",
    "MODEL.gradient_checkpointing_enable()\n",
    "PAD_TOKEN_ID = TOKENIZER.pad_token_id\n",
    "DOCSEP_TOKEN_ID = TOKENIZER.convert_tokens_to_ids(\"<doc-sep>\")\n",
    "print(PAD_TOKEN_ID, DOCSEP_TOKEN_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ec39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=load_dataset('multi_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f6c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def process_document(documents):\n",
    "    input_ids_all=[]\n",
    "    for data in documents:\n",
    "        all_docs = data.split(\"|||||\")\n",
    "        for i, doc in enumerate(all_docs):\n",
    "            doc = doc.replace(\"\\n\", \" \")\n",
    "            doc = \" \".join(doc.split())\n",
    "            all_docs[i] = doc\n",
    "\n",
    "        #### concat with global attention on doc-sep\n",
    "        input_ids = []\n",
    "        for doc in all_docs:\n",
    "            input_ids.extend(\n",
    "                TOKENIZER.encode(\n",
    "                    doc,\n",
    "                    truncation=True,\n",
    "                    max_length=4096 // len(all_docs),\n",
    "                )[1:-1]\n",
    "            )\n",
    "            input_ids.append(DOCSEP_TOKEN_ID)\n",
    "        input_ids = (\n",
    "            [TOKENIZER.bos_token_id]\n",
    "            + input_ids\n",
    "            + [TOKENIZER.eos_token_id]\n",
    "        )\n",
    "        input_ids_all.append(torch.tensor(input_ids))\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_all, batch_first=True, padding_value=PAD_TOKEN_ID\n",
    "    )\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def batch_process(batch):\n",
    "    input_ids=process_document(batch['document'])\n",
    "    \n",
    "    \n",
    "    input_ids = input_ids.to(device)\n",
    "    # get the input ids and attention masks together\n",
    "    global_attention_mask = torch.zeros_like(input_ids).to(input_ids.device)\n",
    "    # put global attention on <s> token\n",
    "\n",
    "    global_attention_mask[:, 0] = 1\n",
    "    global_attention_mask[input_ids == DOCSEP_TOKEN_ID] = 1\n",
    "\n",
    "    start_time = time.time()\n",
    "    generated_ids = MODEL.generate(\n",
    "        input_ids=input_ids,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "        use_cache=False,\n",
    "        max_length=1024,\n",
    "        num_beams=5,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(\"generate time: \", end_time - start_time)\n",
    "    \n",
    "    generated_str = TOKENIZER.batch_decode(\n",
    "            generated_ids.tolist(), skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "    result={}\n",
    "    result['generated_summaries'] = generated_str\n",
    "    result['gt_summaries']=batch['summary']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0145586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]Input ids are automatically padded from 1750 to 2048 to be a multiple of `config.attention_window`: 512\n",
      "Map:  20%|██        | 2/10 [00:58<03:55, 29.43s/ examples]Input ids are automatically padded from 614 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time:  58.794923305511475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  40%|████      | 4/10 [01:10<01:33, 15.57s/ examples]Input ids are automatically padded from 3408 to 3584 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time:  11.716407060623169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  60%|██████    | 6/10 [04:23<03:29, 52.40s/ examples]Input ids are automatically padded from 1331 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time:  192.3845944404602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  80%|████████  | 8/10 [05:08<01:21, 40.61s/ examples]Input ids are automatically padded from 1612 to 2048 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time:  44.86605954170227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [06:19<00:00, 37.91s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate time:  70.8754575252533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "data_idx = random.choices(range(len(dataset['test'])),k=10)\n",
    "# data_idx = range(len(dataset['test']))\n",
    "dataset_small = dataset['test'].select(data_idx)\n",
    "result_small = dataset_small.map(batch_process, batched=True, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7539bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.44424478932154754, recall=0.5022607729006733, fmeasure=0.45485686507700906)\n",
      "Score(precision=0.13797548168492504, recall=0.14655468322949716, fmeasure=0.1373016405017336)\n",
      "Score(precision=0.19899726584511804, recall=0.22245682634806052, fmeasure=0.20221953006844948)\n"
     ]
    }
   ],
   "source": [
    "score=rouge.compute(predictions=result_small[\"generated_summaries\"], references=result_small[\"gt_summaries\"], use_stemmer=True)\n",
    "print(score['rouge1'].mid)\n",
    "print(score['rouge2'].mid)\n",
    "print(score['rougeL'].mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29b599d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047875354107648725"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15210/317700"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mds]",
   "language": "python",
   "name": "conda-env-mds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
